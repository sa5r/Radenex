{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LIMIT = 3000 # 500, 1000 , 1500 , 3000\n",
    "PREDICTION_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('reports_fixed.json')\n",
    "json_data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "all_MeSH = []\n",
    "all_words = []\n",
    "max_sentence_length = 0\n",
    "all_lengths = []\n",
    "\n",
    "for i in json_data:\n",
    "  MeSH = str (json_data[i]['MeSH']).lower()\n",
    "  MeSH_list = MeSH.split(';')\n",
    "  all_MeSH.extend(MeSH_list)\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "  # standardize='lower_and_strip_punctuation',\n",
    "  standardize='lower',\n",
    "  split=None,\n",
    ")\n",
    "vectorize_layer.adapt(all_MeSH)\n",
    "\n",
    "entities_vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in json_data:\n",
    "  findings = str (json_data[i]['findings']).lower()\n",
    "  impression = str (json_data[i]['impression']).lower()\n",
    "  MeSH = str (json_data[i]['MeSH']).lower()\n",
    "\n",
    "  MeSH_list = MeSH.split(';')\n",
    "  sentence = findings + '. ' + impression\n",
    "  sentence_splitted = sentence.split(' ')\n",
    "  all_words.extend(sentence_splitted)\n",
    "  if len(sentence_splitted) > max_sentence_length :\n",
    "    all_lengths.append(len(sentence_splitted))\n",
    "    max_sentence_length = len(sentence_splitted)\n",
    "\n",
    "  entities = [0] * vectorize_layer.vocabulary_size()\n",
    "  for m in MeSH_list :\n",
    "    ent_id = entities_vocabulary.index(m)\n",
    "    entities[ent_id] = 1\n",
    "  \n",
    "  X.append(sentence)\n",
    "  Y.append(entities)\n",
    "\n",
    "  if len(X) > LIMIT :\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_layer.get_vocabulary()\n",
    "# df = pd.DataFrame(all_MeSH)\n",
    "# df.drop_duplicates(inplace = True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "compine_string=(\" \").join(all_MeSH)\n",
    "\n",
    "wordcloud = WordCloud().generate(compine_string)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize = (20,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "compine_string=(\" \").join(all_words)\n",
    "\n",
    "wordcloud = WordCloud().generate(compine_string)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize = (20,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total 2472\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "print(f'X_train size {len(X_train)}')\n",
    "print(f'X_test size {len(X_test)}')\n",
    "print(f'y_train size {len(y_train)}')\n",
    "print(f'y_test size {len(y_test)}')\n",
    "print(f'max_sentence_length {(max_sentence_length)}')\n",
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entities chart\n",
    "mesh_df=pd.DataFrame(all_MeSH)\n",
    "tags_grouped = mesh_df.groupby(mesh_df[0])\n",
    "mesh_df=tags_grouped.size().sort_values()\n",
    "mesh_df = mesh_df.tail(20)\n",
    "mesh_df.plot.bar(figsize=(15,5))\n",
    "# for b in mesh_df.keys():\n",
    "    # print(f\"{b} : {mesh_df[b]}\")\n",
    "# mesh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words chart\n",
    "words_df=pd.DataFrame(all_words)\n",
    "tags_grouped = words_df.groupby(words_df[0])\n",
    "words_df=tags_grouped.size().sort_values()\n",
    "words_df = words_df.tail(20)\n",
    "words_df.plot.bar(figsize=(15,5))\n",
    "# for b in mesh_df.keys():\n",
    "    # print(f\"{b} : {mesh_df[b]}\")\n",
    "# mesh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess = hub.load(\n",
    "  'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    ")\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4\",\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_berted(X):\n",
    "  tok = bert_preprocess.tokenize(X)\n",
    "  text_preprocessed = bert_preprocess.bert_pack_inputs([tok], max_sentence_length)\n",
    "  return {\n",
    "    'input_word_ids': text_preprocessed['input_word_ids'],\n",
    "    'input_mask': text_preprocessed['input_mask'],\n",
    "    'input_type_ids': text_preprocessed['input_type_ids'],\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_berted = get_X_berted(X_train)\n",
    "X_test_berted = get_X_berted(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(tf.keras.losses.Loss):\n",
    "  def __init__(self, smooth=1e-6, gama=2):\n",
    "    super(DiceLoss, self).__init__()\n",
    "    self.name = 'NDL'\n",
    "    self.smooth = smooth\n",
    "    self.gama = gama\n",
    "\n",
    "  def call(self, y_true, y_pred):\n",
    "    y_true, y_pred = tf.cast(\n",
    "        y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
    "    nominator = 2 * \\\n",
    "        tf.reduce_sum(tf.multiply(y_pred, y_true)) + self.smooth\n",
    "    denominator = tf.reduce_sum(\n",
    "        y_pred ** self.gama) + tf.reduce_sum(y_true ** self.gama) + self.smooth\n",
    "    result = 1 - tf.divide(nominator, denominator)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_radenex_model():\n",
    "  input_ids = layers.Input(shape=(max_sentence_length,), dtype=tf.int32)\n",
    "  token_type_ids = layers.Input(shape=(max_sentence_length,), dtype=tf.int32)\n",
    "  attention_mask = layers.Input(shape=(max_sentence_length,), dtype=tf.int32)\n",
    "\n",
    "  embedding = encoder(\n",
    "    dict(input_word_ids=input_ids,input_mask=attention_mask,input_type_ids=token_type_ids),\n",
    "  training=False,\n",
    "  )[\"sequence_output\"]\n",
    "\n",
    "  bilstm = layers.Bidirectional(\n",
    "    layers.LSTM(50, return_sequences=True,\n",
    "    )\n",
    "  )(embedding)\n",
    "\n",
    "  avg1 = layers.AveragePooling1D(pool_size=64,strides=4,padding='same')(bilstm)\n",
    "\n",
    "  flt = layers.Flatten()(avg1)\n",
    "\n",
    "  dropout_layer1 = layers.Dropout(0.25)(flt)\n",
    "\n",
    "  output_layer = layers.Dense(vectorize_layer.vocabulary_size(), activation='sigmoid')(dropout_layer1)\n",
    "\n",
    "  model = tf.keras.Model(\n",
    "    inputs = [input_ids, attention_mask, token_type_ids, ],\n",
    "    outputs = [output_layer],\n",
    "  )\n",
    "  model.compile(optimizer = 'adam', loss = 'binary_crossentropy' ) #categorical_crossentropy binary_crossentropy\n",
    "  print(model.summary())\n",
    "  tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)\n",
    "  return model\n",
    "\n",
    "radenex = create_radenex_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5,\n",
    "            min_delta=1e-3 , # Minimum change in the monitored quantity to qualify as an improvement\n",
    "             baseline = 0.8) # will stop if stays above baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = radenex.fit(\n",
    "      x=[X_train_berted['input_word_ids'], X_train_berted['input_mask'], X_train_berted['input_type_ids'], ],\n",
    "      y=np.array(y_train),\n",
    "      epochs=EPOCHS,\n",
    "      validation_split=0.05,\n",
    "      verbose=1,\n",
    "      callbacks=[callback, ],\n",
    "      # workers=WORKERS,\n",
    "      # use_multiprocessing = True,\n",
    "              )\n",
    "pd.DataFrame(training_history.history).plot(figsize=(12, 10))\n",
    "plt.savefig('history/h_'+datetime.now().strftime('%Y-%m-%d-%H:%M')+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate1d_old(test_Y,predicted):\n",
    "  TP = 0\n",
    "  TN = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  failed_samples = []\n",
    "  for i in range(len(predicted)):\n",
    "    for j in range(len(predicted[i])):\n",
    "      if float( predicted[i][j] ) >= PREDICTION_THRESHOLD and test_Y[i][j] == 1 :\n",
    "        TP += 1\n",
    "      elif float( predicted[i][j] ) < PREDICTION_THRESHOLD and test_Y[i][j] == 1 :\n",
    "        FP += 1\n",
    "        failed_samples.append(i)\n",
    "      elif float( predicted[i][j] ) >= PREDICTION_THRESHOLD and test_Y[i][j] == 0 :\n",
    "        FN += 1\n",
    "        failed_samples.append(i)\n",
    "      else:\n",
    "        TN +=1\n",
    "  print('TP , TN , FP , FN:',TP , TN , FP , FN)\n",
    "  if TP == 0 or TN == 0 or FP == 0 or FN == 0 :\n",
    "    return None\n",
    "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "  precision = TP / (TP + FP)\n",
    "  recall = TP / (TP + FN)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  print('accuracy,precision,recall,f1',accuracy,precision,recall,f1)\n",
    "  return failed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = radenex.predict(\n",
    "      x=[X_test_berted['input_word_ids'], X_test_berted['input_mask'], X_test_berted['input_type_ids'], ], )\n",
    "evaluate1d_old(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b5139d9343808a35adfa94ea45b70d7c400e03bb697fd21693bbcd2dbce3881"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('posbertgpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
